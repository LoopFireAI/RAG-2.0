"""
Final Comprehensive Chroma Database Analysis
This script properly analyzes the ingested documents using the correct Chroma schema.
"""

import sqlite3
import json
from collections import Counter, defaultdict
from datetime import datetime
import os

def comprehensive_chroma_analysis():
    """Perform comprehensive analysis of the Chroma database."""
    
    print("üîç COMPREHENSIVE CHROMA DATABASE ANALYSIS")
    print("=" * 60)
    
    db_path = "./chroma_db/chroma.sqlite3"
    
    if not os.path.exists(db_path):
        print(f"‚ùå Database file not found at: {db_path}")
        return
    
    try:
        conn = sqlite3.connect(db_path)
        cursor = conn.cursor()
        
        print(f"üìä DATABASE OVERVIEW")
        print(f"Database file: {db_path}")
        print(f"File size: {os.path.getsize(db_path):,} bytes ({os.path.getsize(db_path)/1024/1024:.1f} MB)")
        
        # Get basic counts
        cursor.execute("SELECT COUNT(*) FROM embeddings;")
        total_embeddings = cursor.fetchone()[0]
        
        cursor.execute("SELECT COUNT(*) FROM collections;")
        total_collections = cursor.fetchone()[0]
        
        print(f"Total collections: {total_collections}")
        print(f"Total embeddings: {total_embeddings:,}")
        
        # Analyze document content and metadata
        print(f"\nüìà DOCUMENT CONTENT ANALYSIS")
        print("-" * 40)
        
        # Get document content (stored in chroma:document key)
        cursor.execute("""
            SELECT e.id, em_doc.string_value as document, em_source.string_value as source
            FROM embeddings e
            LEFT JOIN embedding_metadata em_doc ON e.id = em_doc.id AND em_doc.key = 'chroma:document'
            LEFT JOIN embedding_metadata em_source ON e.id = em_source.id AND em_source.key = 'source'
            LIMIT 4921
        """)
        
        documents_data = cursor.fetchall()
        print(f"Retrieved document data for {len(documents_data):,} embeddings")
        
        # Analyze sources and file types
        sources = []
        file_types = Counter()
        pdf_files = []
        unique_sources = set()
        doc_lengths = []
        
        for emb_id, document, source in documents_data:
            if source:
                sources.append(source)
                unique_sources.add(source)
                
                # Extract filename from path
                filename = source.split('\\')[-1] if '\\' in source else source.split('/')[-1]
                
                # Determine file type
                if '.' in filename:
                    ext = filename.split('.')[-1].lower()
                    file_types[ext] += 1
                    
                    if ext == 'pdf':
                        pdf_files.append(filename)
            
            if document:
                doc_lengths.append(len(document))
        
        # File type analysis
        print(f"\nüìÅ FILE TYPE DISTRIBUTION:")
        total_docs_with_source = len([s for s in sources if s])
        for file_type, count in file_types.most_common():
            percentage = (count / total_docs_with_source) * 100 if total_docs_with_source > 0 else 0
            print(f"  {file_type.upper()}: {count:,} chunks ({percentage:.1f}%)")
        
        # PDF Analysis
        pdf_count = file_types.get('pdf', 0)
        unique_pdfs = list(set(pdf_files))
        
        print(f"\nüìÑ PDF ANALYSIS:")
        print(f"  Total PDF chunks: {pdf_count:,}")
        print(f"  Unique PDF files: {len(unique_pdfs)}")
        
        if unique_pdfs:
            print(f"\n  üìã PDF FILES FOUND:")
            for i, pdf in enumerate(sorted(unique_pdfs), 1):
                chunk_count = pdf_files.count(pdf)
                print(f"    {i:2d}. {pdf} ({chunk_count} chunks)")
        else:
            print(f"  ‚ùå No PDF files found")
        
        # Get additional metadata statistics
        print(f"\nüè∑Ô∏è  METADATA ANALYSIS:")
        cursor.execute("SELECT key, COUNT(*) as count FROM embedding_metadata GROUP BY key ORDER BY count DESC;")
        metadata_stats = cursor.fetchall()
        
        print(f"  Available metadata fields:")
        for key, count in metadata_stats:
            percentage = (count / total_embeddings) * 100
            print(f"    {key}: {count:,} entries ({percentage:.1f}%)")
        
        # Document content statistics
        if doc_lengths:
            avg_length = sum(doc_lengths) / len(doc_lengths)
            min_length = min(doc_lengths)
            max_length = max(doc_lengths)
            
            print(f"\nüìè DOCUMENT CHUNK STATISTICS:")
            print(f"  Average chunk length: {avg_length:.0f} characters")
            print(f"  Minimum chunk length: {min_length:,} characters")
            print(f"  Maximum chunk length: {max_length:,} characters")
            print(f"  Total content: {sum(doc_lengths):,} characters")
        
        # Sample documents
        print(f"\nüìù SAMPLE DOCUMENTS:")
        print("-" * 25)
        
        cursor.execute("""
            SELECT e.id, em_doc.string_value as document, em_source.string_value as source,
                   em_title.string_value as title, em_page.string_value as page
            FROM embeddings e
            LEFT JOIN embedding_metadata em_doc ON e.id = em_doc.id AND em_doc.key = 'chroma:document'
            LEFT JOIN embedding_metadata em_source ON e.id = em_source.id AND em_source.key = 'source'
            LEFT JOIN embedding_metadata em_title ON e.id = em_title.id AND em_title.key = 'title'
            LEFT JOIN embedding_metadata em_page ON e.id = em_page.id AND em_page.key = 'page'
            LIMIT 5
        """)
        
        samples = cursor.fetchall()
        for i, (emb_id, document, source, title, page) in enumerate(samples, 1):
            filename = source.split('\\')[-1] if source and '\\' in source else (source.split('/')[-1] if source else 'Unknown')
            content_preview = document[:150] + "..." if document and len(document) > 150 else (document or "No content")
            
            print(f"\nSample {i}:")
            print(f"  ID: {emb_id}")
            print(f"  File: {filename}")
            print(f"  Title: {title or 'N/A'}")
            print(f"  Page: {page or 'N/A'}")
            print(f"  Content: {content_preview}")
        
        # Hash verification
        print(f"\nüîê HASH VERIFICATION:")
        print("-" * 20)
        
        hash_file = "ingested_hashes.txt"
        if os.path.exists(hash_file):
            with open(hash_file, 'r') as f:
                hashes = [line.strip() for line in f.readlines() if line.strip()]
            
            print(f"  Total hashes in {hash_file}: {len(hashes)}")
            print(f"  Documents in Chroma DB: {total_embeddings}")
            
            ratio = len(hashes) / total_embeddings if total_embeddings > 0 else 0
            print(f"  Hash to document ratio: {ratio:.2f}")
            
            if len(hashes) > total_embeddings:
                print(f"  ‚ö†Ô∏è  More hashes than DB documents ({len(hashes)} vs {total_embeddings})")
                print(f"      Some documents may have failed to embed")
            elif len(hashes) < total_embeddings:
                print(f"  ‚úÖ Fewer hashes than DB documents ({len(hashes)} vs {total_embeddings})")
                print(f"      This is expected - documents were chunked into multiple embeddings")
            else:
                print(f"  ‚úÖ Hash count matches DB document count")
        else:
            print(f"  ‚ùå Hash file '{hash_file}' not found")
        
        # Final summary
        print(f"\nüìã FINAL SUMMARY:")
        print("=" * 20)
        
        if pdf_count > 0:
            print(f"‚úÖ PDF VERIFICATION: {len(unique_pdfs)} unique PDF files successfully ingested")
            print(f"   - Total PDF chunks: {pdf_count:,}")
            print(f"   - Average chunks per PDF: {pdf_count/len(unique_pdfs):.1f}")
        else:
            print(f"‚ùå PDF VERIFICATION: No PDF files found in database")
        
        print(f"‚úÖ INGESTION STATUS: Successfully processed {len(unique_sources)} unique documents")
        print(f"‚úÖ CHUNKING STATUS: Created {total_embeddings:,} searchable chunks")
        print(f"‚úÖ DATABASE STATUS: Operational with {total_collections} collection(s)")
        
        # Recommendations
        print(f"\nüí° RECOMMENDATIONS:")
        if pdf_count == 0:
            print("   - No PDFs found - check if PDF files were in the source folders")
        elif pdf_count < 100:
            print("   - Low PDF chunk count - verify all PDFs were processed")
        
        if total_embeddings < 1000:
            print("   - Consider adding more documents for better coverage")
        
        print("   - Database is ready for RAG queries")
        print("   - All ingested documents are searchable via embeddings")
        
        # Generate comprehensive report
        generate_comprehensive_report(
            total_embeddings, len(unique_sources), file_types, 
            unique_pdfs, pdf_count, metadata_stats, unique_sources
        )
        
        conn.close()
        
    except Exception as e:
        print(f"‚ùå Error during analysis: {e}")
        import traceback
        traceback.print_exc()
        if 'conn' in locals():
            conn.close()

def generate_comprehensive_report(total_embeddings, unique_sources_count, file_types, 
                                unique_pdfs, pdf_count, metadata_stats, unique_sources):
    """Generate a comprehensive JSON report."""
    
    report_data = {
        "analysis_timestamp": datetime.now().isoformat(),
        "summary": {
            "total_embeddings": total_embeddings,
            "unique_source_documents": unique_sources_count,
            "total_pdf_files": len(unique_pdfs),
            "total_pdf_chunks": pdf_count,
            "database_operational": True
        },
        "file_type_analysis": {
            "distribution": dict(file_types),
            "total_types": len(file_types)
        },
        "pdf_verification": {
            "pdfs_found": len(unique_pdfs) > 0,
            "unique_pdf_files": len(unique_pdfs),
            "pdf_chunks": pdf_count,
            "pdf_files": sorted(unique_pdfs),
            "average_chunks_per_pdf": pdf_count / len(unique_pdfs) if unique_pdfs else 0
        },
        "metadata_fields": [{"field": key, "count": count} for key, count in metadata_stats],
        "ingestion_verification": {
            "all_documents_embedded": total_embeddings > 0,
            "documents_chunked": total_embeddings > unique_sources_count,
            "ready_for_queries": True
        },
        "source_documents": list(unique_sources)[:50]  # First 50 sources
    }
    
    report_file = f"comprehensive_chroma_report_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
    
    try:
        with open(report_file, 'w') as f:
            json.dump(report_data, f, indent=2, default=str)
        print(f"\nüìÑ Comprehensive report saved to: {report_file}")
    except Exception as e:
        print(f"‚ö†Ô∏è  Could not save report: {e}")

if __name__ == "__main__":
    comprehensive_chroma_analysis()